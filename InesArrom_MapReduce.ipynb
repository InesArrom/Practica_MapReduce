{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InesArrom-MapReduce.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "snAvkt1eY8k9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pràctica MapReduce"
      ],
      "metadata": {
        "id": "7iYPFeO5oO3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instal·lar MapReduce"
      ],
      "metadata": {
        "id": "yns48zzswa-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8DjA6bbva8r",
        "outputId": "767b05cd-8238-435d-a09d-404a95a358eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-15 23:07:05--  https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f8:10a:201a::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500749234 (478M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.0.tar.gz’\n",
            "\n",
            "hadoop-3.3.0.tar.gz 100%[===================>] 477.55M  25.9MB/s    in 19s     \n",
            "\n",
            "2022-02-15 23:07:24 (24.9 MB/s) - ‘hadoop-3.3.0.tar.gz’ saved [500749234/500749234]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimir\n",
        "!tar -xzf hadoop-3.3.0.tar.gz"
      ],
      "metadata": {
        "id": "AvWv4bYBvbHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copiar a /usr/local/\n",
        "!cp -r hadoop-3.3.0/ /usr/local/"
      ],
      "metadata": {
        "id": "ViekAqEovbQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2: Configurar el Hadoop JAVA HOME"
      ],
      "metadata": {
        "id": "T6zCk7Fk6R7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cercam la direcció de Java en la màquina Google Colab\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "id": "byDyaObwvbS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20999ab-2c36-4e28-df7d-3015b1e0ab9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Establim mitjançant codi Python el valor a la variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\""
      ],
      "metadata": {
        "id": "wRZVWpOVvbVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Executar Hadoop"
      ],
      "metadata": {
        "id": "lTGKufvY6xfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.0/bin/hadoop"
      ],
      "metadata": {
        "id": "Hky8pJRKvbXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ed82c6-ca0b-4a9e-c3d5-341d5acd5d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primera part:  Carregar les dades al hdfs"
      ],
      "metadata": {
        "id": "i_VHC-3_oVyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cloudera"
      ],
      "metadata": {
        "id": "snAvkt1eY8k9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**script.sh**\n",
        "\n",
        "creem i obrim un script sh\n",
        "```\n",
        "touch script.sh\n",
        "nano script.sh\n",
        "```\n",
        "\n",
        "- Cream una carpeta i ens ubiquem a ella\n",
        "- Instal·lem 5 llibres de  www.gutemberg.org\n",
        "- Carreguem els llibres al hdfs\n",
        "\n",
        "```\n",
        "#! /bin/bash\n",
        "\n",
        "sudo mkdir books\n",
        "cd books\n",
        "\n",
        "wget https://www.gutenberg.org/files/57654/57654-0.txt\n",
        "wget https://www.gutenberg.org/cache/epub/22045/pg22045.txt\n",
        "wget https://www.gutenberg.org/files/61339/61339-0.txt\n",
        "wget https://www.gutenberg.org/cache/epub/29640/pg29640.txt\n",
        "wget https://www.gutenberg.org/cache/epub/54430/pg54430.txt\n",
        "\n",
        "hdfs dfs -put /home/cloudera/books* /user/cloudera\n",
        "hdfs dfs -ls books\n",
        "\n",
        "```\n",
        "\n",
        "- executariem l'script amb \n",
        "```\n",
        "./script.sh\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SO5qevEg7Sj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab"
      ],
      "metadata": {
        "id": "Sm0AKnmuZC9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creem una carpeta\n",
        "!mkdir books"
      ],
      "metadata": {
        "id": "recbwcG0ZQNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd books"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWinmiRwoKe2",
        "outputId": "e5ff6298-f03c-46de-c7ea-19bc859c5ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/books\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarreguem 5 llibres\n",
        "!wget https://www.gutenberg.org/files/57654/57654-0.txt\n",
        "!wget https://www.gutenberg.org/cache/epub/22045/pg22045.txt\n",
        "!wget https://www.gutenberg.org/files/61339/61339-0.txt\n",
        "!wget https://www.gutenberg.org/cache/epub/29640/pg29640.txt\n",
        "!wget https://www.gutenberg.org/cache/epub/54430/pg54430.txt"
      ],
      "metadata": {
        "id": "jr4WwFM7ZpCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada6078a-6de8-4337-c64a-181a3327a509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-15 23:08:49--  https://www.gutenberg.org/files/57654/57654-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1131399 (1.1M) [text/plain]\n",
            "Saving to: ‘57654-0.txt’\n",
            "\n",
            "57654-0.txt         100%[===================>]   1.08M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-15 23:08:49 (8.99 MB/s) - ‘57654-0.txt’ saved [1131399/1131399]\n",
            "\n",
            "--2022-02-15 23:08:50--  https://www.gutenberg.org/cache/epub/22045/pg22045.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149982 (146K) [text/plain]\n",
            "Saving to: ‘pg22045.txt’\n",
            "\n",
            "pg22045.txt         100%[===================>] 146.47K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-02-15 23:08:50 (1.76 MB/s) - ‘pg22045.txt’ saved [149982/149982]\n",
            "\n",
            "--2022-02-15 23:08:50--  https://www.gutenberg.org/files/61339/61339-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 374326 (366K) [text/plain]\n",
            "Saving to: ‘61339-0.txt’\n",
            "\n",
            "61339-0.txt         100%[===================>] 365.55K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-02-15 23:08:50 (4.42 MB/s) - ‘61339-0.txt’ saved [374326/374326]\n",
            "\n",
            "--2022-02-15 23:08:50--  https://www.gutenberg.org/cache/epub/29640/pg29640.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 424276 (414K) [text/plain]\n",
            "Saving to: ‘pg29640.txt’\n",
            "\n",
            "pg29640.txt         100%[===================>] 414.33K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-15 23:08:50 (4.05 MB/s) - ‘pg29640.txt’ saved [424276/424276]\n",
            "\n",
            "--2022-02-15 23:08:50--  https://www.gutenberg.org/cache/epub/54430/pg54430.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 733535 (716K) [text/plain]\n",
            "Saving to: ‘pg54430.txt’\n",
            "\n",
            "pg54430.txt         100%[===================>] 716.34K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-15 23:08:50 (6.94 MB/s) - ‘pg54430.txt’ saved [733535/733535]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprovem que s'han descarregat\n",
        "!ls"
      ],
      "metadata": {
        "id": "NVYP6lQpZpEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883ef671-2c83-492a-bbf4-b245cca00a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57654-0.txt  61339-0.txt  pg22045.txt  pg29640.txt  pg54430.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_9F32khpXcF",
        "outputId": "654ea644-4ca9-4115-9d65-b282afaffef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segona part. Programa map reduce"
      ],
      "metadata": {
        "id": "osAJfARiaUXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per realitzar aquesta part m'he ajudat dels Colabs del Moodle i d'alguns exemples trobats a internet com:\n",
        "\n",
        "- https://www.geeksforgeeks.org/hadoop-streaming-using-python-word-count-problem/"
      ],
      "metadata": {
        "id": "_tEvqWZtikcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mapper.py**\n",
        "\n",
        "Llegirà les dades de STDIN i dividirà les línies en paraules, generarà una sortida de la primera lletra de cada paraula\n",
        "\n",
        "```\n",
        "# importam sys per llegir i escriure a STDIN i STDOUT\n",
        "from nltk.corpus import stopwords\n",
        "import sys\n",
        "import io\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# signes que volem evitar\n",
        "signs = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "# Creem les stopwords dels idiomes que tenim en els llibres\n",
        "stop_words = set(stopwords.words('catalan') + stopwords.words('spanish'))\n",
        "input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')\n",
        "\n",
        "for line in input_stream:\n",
        "    # eliminem els espais en blanc del principi i final\n",
        "    line = line.strip()\n",
        "    line = re.sub(r'[^\\w\\s]', '', line)\n",
        "    # convertim tot en minúscules\n",
        "    line = line.lower()\n",
        "    # sustituim els signes per espais\n",
        "    for char in line:\n",
        "        if char in signs:\n",
        "            line = line.replace(char, \" \")\n",
        "\n",
        "    # De cada paraula comprovem que no sigui una stopword\n",
        "    words = line.split()\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            # primera lletra de la paraula\n",
        "            letter = word[0:1]\n",
        "            # si la lletra es troba a la llista farem el seu compteig\n",
        "            if letter in list('abcdefghijklmnñopqrstuvwxyzç'):\n",
        "                print('%s\\t%s' % (letter, 1))\n",
        "```"
      ],
      "metadata": {
        "id": "tAq1YnB6ahcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reducer.py**\n",
        "\n",
        "Implementa la lògica de reducció. Llegirà la sortida de mapper.py des de STDIN i afegirà l'aparició de cada lletra i escriurà la sortida final a STDOUT\n",
        "\n",
        "```\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    line = line.lower()\n",
        "\n",
        "    # parse the input we got from mapper.py\n",
        "    word, count = line.split('\\t', 1)\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        # count was not a number, ignore/discard this line\n",
        "        continue\n",
        "\n",
        "    # this IF-switch only works because Hadoop sorts map output\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            # write result to STDOUT\n",
        "            print('%s\\t%s' % (current_word, current_count))\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "\n",
        "# output the last word\n",
        "if current_word == word:\n",
        "    print('%s\\t%s' % (current_word, current_count))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Y04eJCCKer3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tercera part: Executar l’aplicació MapReduce "
      ],
      "metadata": {
        "id": "iomL5Qsdgf_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarregam el mapper.py del github\n",
        "!wget https://raw.githubusercontent.com/InesArrom/mapreduce/main/mapper.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbzQeHOSZQdW",
        "outputId": "19c5f29b-7b8f-4553-c2ff-76b4fd7c46e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-15 23:09:32--  https://raw.githubusercontent.com/InesArrom/mapreduce/main/mapper.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1243 (1.2K) [text/plain]\n",
            "Saving to: ‘mapper.py’\n",
            "\n",
            "\rmapper.py             0%[                    ]       0  --.-KB/s               \rmapper.py           100%[===================>]   1.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-15 23:09:32 (70.5 MB/s) - ‘mapper.py’ saved [1243/1243]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarregam el reducer.py del github\n",
        "!wget https://raw.githubusercontent.com/InesArrom/mapreduce/main/reducer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFif-3t1ZQf5",
        "outputId": "0e2b4655-1363-470f-d546-f0517446696d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-15 23:09:35--  https://raw.githubusercontent.com/InesArrom/mapreduce/main/reducer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 871 [text/plain]\n",
            "Saving to: ‘reducer.py’\n",
            "\n",
            "\rreducer.py            0%[                    ]       0  --.-KB/s               \rreducer.py          100%[===================>]     871  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-15 23:09:35 (46.7 MB/s) - ‘reducer.py’ saved [871/871]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cercem la ruta per executar map reduce\n",
        "!find / -name 'hadoop-streaming*.jar'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciesu3PYZpG2",
        "outputId": "73d22217-f40d-4f22-dec1-0fb56340c95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar\n",
            "/usr/local/hadoop-3.3.0/share/hadoop/tools/sources/hadoop-streaming-3.3.0-sources.jar\n",
            "/usr/local/hadoop-3.3.0/share/hadoop/tools/sources/hadoop-streaming-3.3.0-test-sources.jar\n",
            "find: ‘/proc/38/task/38/net’: Invalid argument\n",
            "find: ‘/proc/38/net’: Invalid argument\n",
            "/content/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar\n",
            "/content/hadoop-3.3.0/share/hadoop/tools/sources/hadoop-streaming-3.3.0-sources.jar\n",
            "/content/hadoop-3.3.0/share/hadoop/tools/sources/hadoop-streaming-3.3.0-test-sources.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Executem el mapreduce\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input /content/books -output /content/output -file /content/mapper.py  -file /content/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'"
      ],
      "metadata": {
        "id": "wK13xO9hkmLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d9d6a4-e96d-420c-a5c4-cbcb224f4bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-15 23:10:01,247 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/mapper.py, /content/reducer.py] [] /tmp/streamjob15741950919872639929.jar tmpDir=null\n",
            "2022-02-15 23:10:02,316 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-02-15 23:10:02,599 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-02-15 23:10:02,599 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-02-15 23:10:02,622 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-02-15 23:10:02,838 INFO mapred.FileInputFormat: Total input files to process : 5\n",
            "2022-02-15 23:10:02,863 INFO mapreduce.JobSubmitter: number of splits:5\n",
            "2022-02-15 23:10:03,366 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1883455017_0001\n",
            "2022-02-15 23:10:03,366 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-02-15 23:10:03,862 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1883455017_0001_150c122b-30cf-47ac-b15b-3351b5061d1d/mapper.py\n",
            "2022-02-15 23:10:03,903 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1883455017_0001_7fd3cbb6-10b0-4a04-ab00-a57a7a13633d/reducer.py\n",
            "2022-02-15 23:10:04,027 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-02-15 23:10:04,029 INFO mapreduce.Job: Running job: job_local1883455017_0001\n",
            "2022-02-15 23:10:04,037 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-02-15 23:10:04,039 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-02-15 23:10:04,049 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-02-15 23:10:04,049 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-02-15 23:10:04,112 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-02-15 23:10:04,116 INFO mapred.LocalJobRunner: Starting task: attempt_local1883455017_0001_m_000000_0\n",
            "2022-02-15 23:10:04,153 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-02-15 23:10:04,155 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-02-15 23:10:04,201 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-02-15 23:10:04,210 INFO mapred.MapTask: Processing split: file:/content/books/57654-0.txt:0+1131399\n",
            "2022-02-15 23:10:04,227 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-02-15 23:10:04,302 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-02-15 23:10:04,302 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-02-15 23:10:04,302 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-02-15 23:10:04,302 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-02-15 23:10:04,302 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-02-15 23:10:04,308 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-02-15 23:10:04,321 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n",
            "2022-02-15 23:10:04,327 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-02-15 23:10:04,328 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-02-15 23:10:04,333 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-02-15 23:10:04,333 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-02-15 23:10:04,334 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-02-15 23:10:04,334 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-02-15 23:10:04,335 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-02-15 23:10:04,335 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-02-15 23:10:04,336 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-02-15 23:10:04,337 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-02-15 23:10:04,337 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-02-15 23:10:04,337 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-02-15 23:10:04,358 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
            "2022-02-15 23:10:04,358 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:04,358 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:04,361 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:04,379 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:05,035 INFO mapreduce.Job: Job job_local1883455017_0001 running in uber mode : false\n",
            "2022-02-15 23:10:05,038 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-02-15 23:10:05,986 INFO streaming.PipeMapRed: Records R/W=2358/1\n",
            "2022-02-15 23:10:06,303 INFO streaming.PipeMapRed: R/W/S=10000/38921/0 in:10000=10000/1 [rec/s] out:38921=38921/1 [rec/s]\n",
            "2022-02-15 23:10:06,771 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-02-15 23:10:06,772 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-02-15 23:10:06,775 INFO mapred.LocalJobRunner: \n",
            "2022-02-15 23:10:06,775 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-02-15 23:10:06,775 INFO mapred.MapTask: Spilling map output\n",
            "2022-02-15 23:10:06,775 INFO mapred.MapTask: bufstart = 0; bufend = 385176; bufvoid = 104857600\n",
            "2022-02-15 23:10:06,775 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25829224(103316896); length = 385173/6553600\n",
            "2022-02-15 23:10:07,026 INFO mapred.MapTask: Finished spill 0\n",
            "2022-02-15 23:10:07,048 INFO mapred.Task: Task:attempt_local1883455017_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-02-15 23:10:07,052 INFO mapred.LocalJobRunner: Records R/W=2358/1\n",
            "2022-02-15 23:10:07,053 INFO mapred.Task: Task 'attempt_local1883455017_0001_m_000000_0' done.\n",
            "2022-02-15 23:10:07,061 INFO mapred.Task: Final Counters for attempt_local1883455017_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1135357\n",
            "\t\tFILE: Number of bytes written=1196831\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=20237\n",
            "\t\tMap output records=96294\n",
            "\t\tMap output bytes=385176\n",
            "\t\tMap output materialized bytes=577770\n",
            "\t\tInput split bytes=83\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=96294\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=275775488\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1131399\n",
            "2022-02-15 23:10:07,063 INFO mapred.LocalJobRunner: Finishing task: attempt_local1883455017_0001_m_000000_0\n",
            "2022-02-15 23:10:07,064 INFO mapred.LocalJobRunner: Starting task: attempt_local1883455017_0001_m_000001_0\n",
            "2022-02-15 23:10:07,067 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-02-15 23:10:07,067 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-02-15 23:10:07,067 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-02-15 23:10:07,069 INFO mapred.MapTask: Processing split: file:/content/books/pg54430.txt:0+733535\n",
            "2022-02-15 23:10:07,073 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-02-15 23:10:07,119 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-02-15 23:10:07,119 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-02-15 23:10:07,120 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-02-15 23:10:07,120 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-02-15 23:10:07,120 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-02-15 23:10:07,127 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-02-15 23:10:07,149 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n",
            "2022-02-15 23:10:07,174 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
            "2022-02-15 23:10:07,174 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:07,175 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:07,175 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:07,180 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:08,047 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2022-02-15 23:10:08,084 INFO streaming.PipeMapRed: Records R/W=2894/1\n",
            "2022-02-15 23:10:08,262 INFO streaming.PipeMapRed: R/W/S=10000/26629/0 in:10000=10000/1 [rec/s] out:26629=26629/1 [rec/s]\n",
            "2022-02-15 23:10:08,554 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-02-15 23:10:08,555 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-02-15 23:10:08,555 INFO mapred.LocalJobRunner: \n",
            "2022-02-15 23:10:08,555 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-02-15 23:10:08,555 INFO mapred.MapTask: Spilling map output\n",
            "2022-02-15 23:10:08,555 INFO mapred.MapTask: bufstart = 0; bufend = 251284; bufvoid = 104857600\n",
            "2022-02-15 23:10:08,555 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25963120(103852480); length = 251277/6553600\n",
            "2022-02-15 23:10:08,629 INFO mapred.MapTask: Finished spill 0\n",
            "2022-02-15 23:10:08,639 INFO mapred.Task: Task:attempt_local1883455017_0001_m_000001_0 is done. And is in the process of committing\n",
            "2022-02-15 23:10:08,641 INFO mapred.LocalJobRunner: Records R/W=2894/1\n",
            "2022-02-15 23:10:08,641 INFO mapred.Task: Task 'attempt_local1883455017_0001_m_000001_0' done.\n",
            "2022-02-15 23:10:08,643 INFO mapred.Task: Final Counters for attempt_local1883455017_0001_m_000001_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1869326\n",
            "\t\tFILE: Number of bytes written=1573793\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=16579\n",
            "\t\tMap output records=62820\n",
            "\t\tMap output bytes=251284\n",
            "\t\tMap output materialized bytes=376930\n",
            "\t\tInput split bytes=83\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=62820\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=18\n",
            "\t\tTotal committed heap usage (bytes)=275775488\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=733535\n",
            "2022-02-15 23:10:08,643 INFO mapred.LocalJobRunner: Finishing task: attempt_local1883455017_0001_m_000001_0\n",
            "2022-02-15 23:10:08,643 INFO mapred.LocalJobRunner: Starting task: attempt_local1883455017_0001_m_000002_0\n",
            "2022-02-15 23:10:08,646 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-02-15 23:10:08,646 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-02-15 23:10:08,646 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-02-15 23:10:08,647 INFO mapred.MapTask: Processing split: file:/content/books/pg29640.txt:0+424276\n",
            "2022-02-15 23:10:08,656 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-02-15 23:10:08,697 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-02-15 23:10:08,697 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-02-15 23:10:08,697 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-02-15 23:10:08,697 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-02-15 23:10:08,697 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-02-15 23:10:08,700 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-02-15 23:10:08,720 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n",
            "2022-02-15 23:10:08,763 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
            "2022-02-15 23:10:08,763 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:08,763 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:08,764 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:08,764 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:09,735 INFO streaming.PipeMapRed: Records R/W=2550/1\n",
            "2022-02-15 23:10:10,001 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-02-15 23:10:10,002 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-02-15 23:10:10,002 INFO mapred.LocalJobRunner: \n",
            "2022-02-15 23:10:10,003 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-02-15 23:10:10,003 INFO mapred.MapTask: Spilling map output\n",
            "2022-02-15 23:10:10,003 INFO mapred.MapTask: bufstart = 0; bufend = 142740; bufvoid = 104857600\n",
            "2022-02-15 23:10:10,003 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26071660(104286640); length = 142737/6553600\n",
            "2022-02-15 23:10:10,045 INFO mapred.MapTask: Finished spill 0\n",
            "2022-02-15 23:10:10,048 INFO mapred.Task: Task:attempt_local1883455017_0001_m_000002_0 is done. And is in the process of committing\n",
            "2022-02-15 23:10:10,050 INFO mapred.LocalJobRunner: Records R/W=2550/1\n",
            "2022-02-15 23:10:10,050 INFO mapred.Task: Task 'attempt_local1883455017_0001_m_000002_0' done.\n",
            "2022-02-15 23:10:10,051 INFO mapred.Task: Final Counters for attempt_local1883455017_0001_m_000002_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2294036\n",
            "\t\tFILE: Number of bytes written=1787941\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=8028\n",
            "\t\tMap output records=35685\n",
            "\t\tMap output bytes=142740\n",
            "\t\tMap output materialized bytes=214116\n",
            "\t\tInput split bytes=83\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=35685\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=24\n",
            "\t\tTotal committed heap usage (bytes)=275775488\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=424276\n",
            "2022-02-15 23:10:10,051 INFO mapred.LocalJobRunner: Finishing task: attempt_local1883455017_0001_m_000002_0\n",
            "2022-02-15 23:10:10,051 INFO mapred.LocalJobRunner: Starting task: attempt_local1883455017_0001_m_000003_0\n",
            "2022-02-15 23:10:10,053 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-02-15 23:10:10,053 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-02-15 23:10:10,053 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-02-15 23:10:10,054 INFO mapred.MapTask: Processing split: file:/content/books/61339-0.txt:0+374326\n",
            "2022-02-15 23:10:10,058 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-02-15 23:10:10,084 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-02-15 23:10:10,085 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-02-15 23:10:10,085 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-02-15 23:10:10,085 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-02-15 23:10:10,085 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-02-15 23:10:10,086 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-02-15 23:10:10,096 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n",
            "2022-02-15 23:10:10,119 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
            "2022-02-15 23:10:10,130 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:10,130 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:10,130 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:10,131 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:11,067 INFO streaming.PipeMapRed: Records R/W=2175/1\n",
            "2022-02-15 23:10:11,333 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-02-15 23:10:11,334 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-02-15 23:10:11,334 INFO mapred.LocalJobRunner: \n",
            "2022-02-15 23:10:11,334 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-02-15 23:10:11,335 INFO mapred.MapTask: Spilling map output\n",
            "2022-02-15 23:10:11,335 INFO mapred.MapTask: bufstart = 0; bufend = 133860; bufvoid = 104857600\n",
            "2022-02-15 23:10:11,335 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26080540(104322160); length = 133857/6553600\n",
            "2022-02-15 23:10:11,362 INFO mapred.MapTask: Finished spill 0\n",
            "2022-02-15 23:10:11,366 INFO mapred.Task: Task:attempt_local1883455017_0001_m_000003_0 is done. And is in the process of committing\n",
            "2022-02-15 23:10:11,368 INFO mapred.LocalJobRunner: Records R/W=2175/1\n",
            "2022-02-15 23:10:11,368 INFO mapred.Task: Task 'attempt_local1883455017_0001_m_000003_0' done.\n",
            "2022-02-15 23:10:11,368 INFO mapred.Task: Final Counters for attempt_local1883455017_0001_m_000003_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2668796\n",
            "\t\tFILE: Number of bytes written=1988769\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6601\n",
            "\t\tMap output records=33465\n",
            "\t\tMap output bytes=133860\n",
            "\t\tMap output materialized bytes=200796\n",
            "\t\tInput split bytes=83\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=33465\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=10\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=374326\n",
            "2022-02-15 23:10:11,368 INFO mapred.LocalJobRunner: Finishing task: attempt_local1883455017_0001_m_000003_0\n",
            "2022-02-15 23:10:11,369 INFO mapred.LocalJobRunner: Starting task: attempt_local1883455017_0001_m_000004_0\n",
            "2022-02-15 23:10:11,372 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-02-15 23:10:11,372 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-02-15 23:10:11,373 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-02-15 23:10:11,374 INFO mapred.MapTask: Processing split: file:/content/books/pg22045.txt:0+149982\n",
            "2022-02-15 23:10:11,378 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-02-15 23:10:11,401 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-02-15 23:10:11,401 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-02-15 23:10:11,401 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-02-15 23:10:11,401 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-02-15 23:10:11,401 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-02-15 23:10:11,404 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-02-15 23:10:11,418 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n",
            "2022-02-15 23:10:11,443 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
            "2022-02-15 23:10:11,443 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:11,443 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:11,443 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:11,444 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:12,325 INFO streaming.PipeMapRed: Records R/W=4839/1\n",
            "2022-02-15 23:10:12,528 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-02-15 23:10:12,529 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-02-15 23:10:12,530 INFO mapred.LocalJobRunner: \n",
            "2022-02-15 23:10:12,530 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-02-15 23:10:12,530 INFO mapred.MapTask: Spilling map output\n",
            "2022-02-15 23:10:12,530 INFO mapred.MapTask: bufstart = 0; bufend = 75924; bufvoid = 104857600\n",
            "2022-02-15 23:10:12,530 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26138476(104553904); length = 75921/6553600\n",
            "2022-02-15 23:10:12,540 INFO mapred.MapTask: Finished spill 0\n",
            "2022-02-15 23:10:12,542 INFO mapred.Task: Task:attempt_local1883455017_0001_m_000004_0 is done. And is in the process of committing\n",
            "2022-02-15 23:10:12,547 INFO mapred.LocalJobRunner: Records R/W=4839/1\n",
            "2022-02-15 23:10:12,550 INFO mapred.Task: Task 'attempt_local1883455017_0001_m_000004_0' done.\n",
            "2022-02-15 23:10:12,551 INFO mapred.Task: Final Counters for attempt_local1883455017_0001_m_000004_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2819212\n",
            "\t\tFILE: Number of bytes written=2102693\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=5104\n",
            "\t\tMap output records=18981\n",
            "\t\tMap output bytes=75924\n",
            "\t\tMap output materialized bytes=113892\n",
            "\t\tInput split bytes=83\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=18981\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=4\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=149982\n",
            "2022-02-15 23:10:12,552 INFO mapred.LocalJobRunner: Finishing task: attempt_local1883455017_0001_m_000004_0\n",
            "2022-02-15 23:10:12,552 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-02-15 23:10:12,556 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-02-15 23:10:12,557 INFO mapred.LocalJobRunner: Starting task: attempt_local1883455017_0001_r_000000_0\n",
            "2022-02-15 23:10:12,568 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-02-15 23:10:12,568 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-02-15 23:10:12,569 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-02-15 23:10:12,571 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3dd1c232\n",
            "2022-02-15 23:10:12,572 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-02-15 23:10:12,596 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-02-15 23:10:12,618 INFO reduce.EventFetcher: attempt_local1883455017_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-02-15 23:10:12,668 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1883455017_0001_m_000003_0 decomp: 200792 len: 200796 to MEMORY\n",
            "2022-02-15 23:10:12,672 INFO reduce.InMemoryMapOutput: Read 200792 bytes from map-output for attempt_local1883455017_0001_m_000003_0\n",
            "2022-02-15 23:10:12,676 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 200792, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->200792\n",
            "2022-02-15 23:10:12,682 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1883455017_0001_m_000000_0 decomp: 577766 len: 577770 to MEMORY\n",
            "2022-02-15 23:10:12,684 INFO reduce.InMemoryMapOutput: Read 577766 bytes from map-output for attempt_local1883455017_0001_m_000000_0\n",
            "2022-02-15 23:10:12,684 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 577766, inMemoryMapOutputs.size() -> 2, commitMemory -> 200792, usedMemory ->778558\n",
            "2022-02-15 23:10:12,686 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1883455017_0001_m_000002_0 decomp: 214112 len: 214116 to MEMORY\n",
            "2022-02-15 23:10:12,687 INFO reduce.InMemoryMapOutput: Read 214112 bytes from map-output for attempt_local1883455017_0001_m_000002_0\n",
            "2022-02-15 23:10:12,688 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 214112, inMemoryMapOutputs.size() -> 3, commitMemory -> 778558, usedMemory ->992670\n",
            "2022-02-15 23:10:12,690 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1883455017_0001_m_000001_0 decomp: 376926 len: 376930 to MEMORY\n",
            "2022-02-15 23:10:12,693 INFO reduce.InMemoryMapOutput: Read 376926 bytes from map-output for attempt_local1883455017_0001_m_000001_0\n",
            "2022-02-15 23:10:12,693 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 376926, inMemoryMapOutputs.size() -> 4, commitMemory -> 992670, usedMemory ->1369596\n",
            "2022-02-15 23:10:12,695 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1883455017_0001_m_000004_0 decomp: 113888 len: 113892 to MEMORY\n",
            "2022-02-15 23:10:12,696 INFO reduce.InMemoryMapOutput: Read 113888 bytes from map-output for attempt_local1883455017_0001_m_000004_0\n",
            "2022-02-15 23:10:12,696 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 113888, inMemoryMapOutputs.size() -> 5, commitMemory -> 1369596, usedMemory ->1483484\n",
            "2022-02-15 23:10:12,696 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-02-15 23:10:12,697 INFO mapred.LocalJobRunner: 5 / 5 copied.\n",
            "2022-02-15 23:10:12,697 INFO reduce.MergeManagerImpl: finalMerge called with 5 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-02-15 23:10:12,708 INFO mapred.Merger: Merging 5 sorted segments\n",
            "2022-02-15 23:10:12,710 INFO mapred.Merger: Down to the last merge-pass, with 5 segments left of total size: 1483464 bytes\n",
            "2022-02-15 23:10:12,990 INFO reduce.MergeManagerImpl: Merged 5 segments, 1483484 bytes to disk to satisfy reduce memory limit\n",
            "2022-02-15 23:10:12,991 INFO reduce.MergeManagerImpl: Merging 1 files, 1483480 bytes from disk\n",
            "2022-02-15 23:10:12,992 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-02-15 23:10:12,992 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-02-15 23:10:12,992 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1483472 bytes\n",
            "2022-02-15 23:10:12,997 INFO mapred.LocalJobRunner: 5 / 5 copied.\n",
            "2022-02-15 23:10:13,007 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
            "2022-02-15 23:10:13,012 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-02-15 23:10:13,013 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-02-15 23:10:13,040 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:13,040 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:13,041 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:13,047 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:13,073 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:13,484 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:13,691 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-02-15 23:10:13,777 INFO streaming.PipeMapRed: Records R/W=247245/1\n",
            "2022-02-15 23:10:13,782 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-02-15 23:10:13,783 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-02-15 23:10:13,784 INFO mapred.Task: Task:attempt_local1883455017_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-02-15 23:10:13,785 INFO mapred.LocalJobRunner: 5 / 5 copied.\n",
            "2022-02-15 23:10:13,785 INFO mapred.Task: Task attempt_local1883455017_0001_r_000000_0 is allowed to commit now\n",
            "2022-02-15 23:10:13,786 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1883455017_0001_r_000000_0' to file:/content/output\n",
            "2022-02-15 23:10:13,789 INFO mapred.LocalJobRunner: Records R/W=247245/1 > reduce\n",
            "2022-02-15 23:10:13,789 INFO mapred.Task: Task 'attempt_local1883455017_0001_r_000000_0' done.\n",
            "2022-02-15 23:10:13,790 INFO mapred.Task: Final Counters for attempt_local1883455017_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=5786356\n",
            "\t\tFILE: Number of bytes written=3586378\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=1483504\n",
            "\t\tReduce input records=247245\n",
            "\t\tReduce output records=27\n",
            "\t\tSpilled Records=247245\n",
            "\t\tShuffled Maps =5\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=5\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=205\n",
            "2022-02-15 23:10:13,790 INFO mapred.LocalJobRunner: Finishing task: attempt_local1883455017_0001_r_000000_0\n",
            "2022-02-15 23:10:13,790 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-02-15 23:10:14,055 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-02-15 23:10:14,056 INFO mapreduce.Job: Job job_local1883455017_0001 completed successfully\n",
            "2022-02-15 23:10:14,074 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=16573083\n",
            "\t\tFILE: Number of bytes written=12236405\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=56549\n",
            "\t\tMap output records=247245\n",
            "\t\tMap output bytes=988984\n",
            "\t\tMap output materialized bytes=1483504\n",
            "\t\tInput split bytes=415\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=1483504\n",
            "\t\tReduce input records=247245\n",
            "\t\tReduce output records=27\n",
            "\t\tSpilled Records=494490\n",
            "\t\tShuffled Maps =5\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=5\n",
            "\t\tGC time elapsed (ms)=56\n",
            "\t\tTotal committed heap usage (bytes)=1821376512\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2813518\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=205\n",
            "2022-02-15 23:10:14,074 INFO streaming.StreamJob: Output directory: /content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comprovam al directori output si ens ha creat un arxiu _SUCCESS que ha anat be i l'arixu amb els resultats del mapreduce\n",
        "!ls /content/output"
      ],
      "metadata": {
        "id": "r_qeYai4hnCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5346a2cb-624b-4203-b1b1-c078c8ba9707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualitzem el resultat\n",
        "!cat /content/output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y3NetfZh8uO",
        "outputId": "5ef60ad8-8b60-420b-9b3b-923fc6ef01e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\t25373\n",
            "b\t5695\n",
            "c\t25657\n",
            "d\t21237\n",
            "e\t12864\n",
            "f\t7869\n",
            "g\t5705\n",
            "h\t11195\n",
            "i\t7731\n",
            "j\t3789\n",
            "k\t49\n",
            "l\t8646\n",
            "m\t15811\n",
            "n\t4893\n",
            "o\t6257\n",
            "p\t23641\n",
            "q\t2850\n",
            "r\t9381\n",
            "s\t16390\n",
            "t\t14253\n",
            "u\t1865\n",
            "v\t12792\n",
            "w\t1054\n",
            "x\t1486\n",
            "y\t633\n",
            "z\t125\n",
            "ñ\t4\n"
          ]
        }
      ]
    }
  ]
}